"""
Stellecta LucidAI Backend - LLM Interaction Model

NEW TABLE: Captures all LLM inference requests and responses.

This is a critical component of the Multi-LLM architecture, logging:
- Which LLM was used (LucidAI, Gemini, OpenAI, etc.)
- Routing decisions and policies applied
- Response quality metrics
- Performance data (latency, cost)
- Feedback signals (for RLHF training)
"""

from sqlalchemy import Column, String, Integer, Float, DateTime, Text, ForeignKey, Boolean, JSON
from sqlalchemy.dialects.postgresql import UUID, ARRAY
from sqlalchemy.sql import func
from sqlalchemy.orm import relationship
import uuid

from app.database.engine import Base


class LLMInteraction(Base):
    """
    LLM Interaction entity.

    Captures every LLM inference request and response for:
    - Performance monitoring
    - Training data collection
    - Routing policy optimization
    - Cost tracking
    - Quality evaluation
    """

    __tablename__ = "llm_interactions"

    # Primary Key
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)

    # Foreign Keys
    conversation_id = Column(UUID(as_uuid=True), ForeignKey("conversations.id"), nullable=False, index=True)
    message_id = Column(UUID(as_uuid=True), ForeignKey("conversation_messages.id"), nullable=False, index=True)

    # ========================================================================
    # REQUEST METADATA
    # ========================================================================
    requested_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False, index=True)
    task_type = Column(String(50), nullable=True, index=True)
    """
    Task type for routing decisions:
    - tutoring
    - mastery_verification
    - creative_exploration
    - code_generation
    """

    risk_level = Column(String(20), nullable=True)
    """Risk level: low, medium, high"""

    # ========================================================================
    # ROUTING DECISION
    # ========================================================================
    routing_decision = Column(JSON, nullable=True)
    """
    Full routing decision object:
    {
        "primary_llm": "lucidai",
        "fallback_llm": "gemini",
        "reason": "high_proficiency_student",
        "policy_overrides": {...}
    }
    """

    llms_queried = Column(ARRAY(String), nullable=True)
    """Array of LLMs queried if hybrid mode: ["lucidai", "gemini"]"""

    # ========================================================================
    # RESPONSE METADATA
    # ========================================================================
    llm_used = Column(String(50), nullable=False, index=True)
    """
    LLM that generated the final response:
    - lucidai
    - gemini
    - openai
    - claude
    - perplexity
    - deepseek
    """

    model_version = Column(String(100), nullable=True)
    """
    Specific model version:
    - lucidai-v1.2-stella
    - gemini-2.5-flash
    - gpt-4-turbo-preview
    """

    response_text = Column(Text, nullable=True)
    """LLM-generated response (full text)"""

    confidence_score = Column(Float, nullable=True)
    """Self-reported confidence from LLM (0-1)"""

    # ========================================================================
    # EVALUATION SCORES
    # ========================================================================
    evaluation_scores = Column(JSON, nullable=True)
    """
    Multi-dimensional quality evaluation:
    {
        "correctness": 0.95,
        "didactic_quality": 0.91,
        "persona_alignment": 0.89,
        "safety": 1.0,
        "curriculum_alignment": 0.87
    }
    """

    composite_score = Column(Float, nullable=True)
    """Weighted composite quality score (0-1)"""

    # ========================================================================
    # PERFORMANCE
    # ========================================================================
    inference_time_ms = Column(Integer, nullable=True)
    """LLM inference latency in milliseconds"""

    tokens_input = Column(Integer, nullable=True)
    """Input tokens sent to LLM"""

    tokens_output = Column(Integer, nullable=True)
    """Output tokens generated by LLM"""

    cost_usd = Column(Float, nullable=True)
    """Estimated cost in USD for this interaction"""

    # ========================================================================
    # ALTERNATIVES (Hybrid Mode)
    # ========================================================================
    alternative_responses = Column(JSON, nullable=True)
    """
    If hybrid mode used, store alternative responses:
    [
        {
            "llm": "gemini",
            "response": "...",
            "confidence": 0.82,
            "scores": {...}
        }
    ]
    """

    # ========================================================================
    # FEEDBACK (Populated Later)
    # ========================================================================
    student_helpful = Column(Boolean, nullable=True)
    """Student thumbs up/down feedback"""

    teacher_rating = Column(Integer, nullable=True)
    """Teacher quality rating (1-5 stars)"""

    outcome_h_pem_delta = Column(Float, nullable=True)
    """H-PEM proficiency change after this interaction (for RLHF reward signal)"""

    # ========================================================================
    # TIMESTAMPS
    # ========================================================================
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False)

    # ========================================================================
    # RELATIONSHIPS
    # ========================================================================
    conversation = relationship("Conversation", back_populates="llm_interactions")
    message = relationship("ConversationMessage", back_populates="llm_interaction")
    training_example = relationship("TrainingExample", back_populates="source_interaction", uselist=False)
    performance_tracking = relationship("LLMPerformanceTracking", back_populates="interaction")

    def __repr__(self):
        return f"<LLMInteraction(id={self.id}, llm={self.llm_used}, confidence={self.confidence_score})>"
